{"podcast_details": {"podcast_title": "The Cloudcast", "episode_title": "Understanding Machine Learning Features and Platforms", "episode_image": "https://storage.buzzsprout.com/variants/nbfsc2plb4xxeglxxu31h35v7arg/f6fd9b4ca4e468e7e11c8350463c894b252ee834d352e0a8d889b97ac83aedef?.jpg", "episode_transcript": " This episode is supported by the AWS Insiders podcast. AWS Insiders is a fast-paced, entertaining, and insightful look behind the scenes of AWS and cloud computing. I'm a subscriber to AWS Insiders, and I love Rahul and Hillary's practical analysis and interviews. As an F1 racing fan, the recent episode on how AWS and F1 partnered together to make the races more exciting was especially entertaining. Search for AWS Insiders in your podcast player. We'll also include a link in the show notes. My thanks to AWS Insiders for their support. Cloudcast Media presents from the massive studios in Raleigh, North Carolina. This is the Cloudcast with Aaron Delp and Brian Grace Lee, bringing you the best of cloud computing from around the world. Good morning, good evening, wherever you are. Welcome back to the Cloudcast. We're coming to you live from our massive Cloudcast studios here in Raleigh, North Carolina. And it is Aaron for Cloud News this week. And we're gonna get the quick version this week because I've got a plane to catch. I'm heading out to corporate headquarters this week for some meetings. So we're gonna jump right in to cloud news. The first article, and probably the one that caused the biggest stink within the infrastructure and cloud community, HashiCorp. HashiCorp is ending their open source software licenses for most of their products and adopting the BSL license. And if you're not familiar with BSL, the business source license, it doesn't meet the standards for open source according to the open source initiative. And this was a big deal and caused a lot of hoopla in the industry this week. And lots of critics of it, of course, but also too a trend that we're starting to see more and more. A lot of these big organizations, open source and open source projects was definitely the norm for the last decade or so. But we're seeing more and more open source projects based off of one company's assets starting to make this move because other organizations are taking that and then building services around it or doing other things as well. And then competing with that company, HashiCorp, in this example. So we shall see, and we're gonna follow that one closely. Our second one was probably one of the biggest news stories in the AI space this week was NVIDIA. NVIDIA at SIGGRAPH did a bunch of announcements. And one of them was they partnered with Hugging Face. Now, if you're not familiar with Hugging Face, if you're new to AI, think of it as SaaS infrastructure or SaaS offering that lets developers build, train and deploy models, a very big presence within the AI community. And for those getting started and wanna get started quickly and not necessarily build their models in one public cloud and or on-prem as well. So they partnered with NVIDIA and NVIDIA is going to bring their DGX cloud line of infrastructure and partnered with them so that you can now run this specifically on NVIDIA infrastructure. Really interesting development, two big folks in the industry there partnering together and we'll see where this goes. The third one was more a paper. So this one is definitely some late night bedtime reading and kind of thing longer paper. But I thought it was really fascinating. It's all about defining, measuring and managing technical debt. For those of us that have been in this industry for a while, technical debt is everywhere, right? And we've all seen it in various forms, but this paper actually goes into, let's define exactly what that is. Let's figure out how we actually measure it to see how good or bad it is. And then how do we manage that over time? Very fascinating subject and something that was very timely and can be used practically in our industry. So with that, I'm going to close out cloud news for this week. Coming up after the break, we have Gaitan Kastilain over at Tecton and we're gonna be talking about understanding machine learning features and platforms. And even if you're not super into AI, you'll wanna stick around for this one. We go into a lot of the basics and really start at the very start of AI and terminology and then build up from there all the way from what is a feature and a model all the way to building platforms. So great conversation and I certainly invite you to check that out. And that's coming up after the break. Are you looking to stay ahead of the curve in the latest trends in enterprise tech? Look no further than the Breaking Analysis podcast with Dave Vellante. This data-driven program dives into the most important topics facing the enterprise tech industry today. With the data-first approach that leverages ETR's renowned surveys of IT decision makers and insight from the cube community, Breaking Analysis delivers in-depth research on the most important topics facing technologists and IT buyers. Whether you're a business leader, an IT professional, investor or just an avid follower of the industry, this podcast is a must listen. Just search Breaking Analysis podcast wherever you get your podcast and tune in today to stay ahead of the game in enterprise tech. Today's show is sponsored by Panoptica. Panoptica simplifies container deployment, monitoring and security, protecting the entire application stack from build to runtime. Scalable across clusters and multi-cloud environments, Panoptica secures containers, serverless APIs and Kubernetes with a unified view, reducing operational complexity and promoting collaboration by integrating with commonly used developer, SRE and SecOps tools. Panoptica ensures compliance with regulatory mandates and CIS benchmarks for best practice conformity. Privacy teams can monitor API traffic and identify sensitive data while identifying open source components vulnerable to attacks that require patching. Proactively addressing security issues with Panoptica allows businesses to focus on mitigating critical risks and protecting their interest. Learn more about Panoptica today at panoptica.app. That's panoptica.app. And we're back and Aaron, two weeks in a row, you and I are our co-host in the show. Good to have you back man. How are you, how have you been? Yeah, good, good, good. Yeah, as we mentioned, I mean, summer has been really, really busy for both of us, but it's good to kind of get back in the swing of things here as we're approaching autumn here in the United States. Absolutely, absolutely. One of the things that we've been trying to figure out the best balance of is obviously, AI has been this dominant conversation throughout the year. We are both trying to find ways to kind of, cover a broad kind of portfolio, if you will, a broad picture of what the AI world looks like, but at the same time, not overwhelming people because we know that some people are, deeply ingrained with this. Some people are very new to this. Some people are like, hold on, I do data science, I do machine learning, I do other aspects of it. And so we've been trying to very much focus on, giving people a broad view of what's going on here. And one of the things that I think we often find ourselves trying to do is to make sure that, that we really understand as much depth as we can around both kind of the halo or the umbrella of AI, but also, kind of under the covers, what does a data scientist's life look like? What does it look like when they're building models? What does it look like the platforms they work with and those types of things? And we're really, really excited today to have Gaitan Casalon, who is VP of marketing at Tecton. And we're gonna talk a lot about, machine learning platforms, but also machine learning features. So Gaitan, welcome to the show. Great to have you on today. Thank you very much, Brian. It's great to be on. Before we dive into machine learning, before we dive into Tecton, give us some of your background and really kind of, Tecton has been on the show before, it's been a couple of years. Give us also the focus of the company, what sort of the segment of the market and the technology that you bring to market. Yeah, yeah. So personally, I've been at Tecton now for almost four years. So I joined Tecton very early on. We were, I think seven employees when I first joined. I've been leading marketing, developer relations and alliances at Tecton. Before Tecton, I spent a few years at Confluent and then before that, Cohesity. So I've been in that data space for quite a while and Tecton was like a natural extension going from streaming data at Confluent to enabling and helping data teams build features using streaming data at Tecton. And so that brings me to Tecton. So what does Tecton do? So roughly speaking, you can think of Tecton as a data platform for machine learning, right? So the biggest challenge when building machine learning applications is to get the data right. So the quality of your predictions is always going to depend on the quality of the data that you're feeding to your models. And this becomes particularly challenging as we're doing machine learning in production. But so if all you're doing is batch machine learning, you can kind of use existing tooling to build batch pipelines. But as machine learning makes its way to production, it really belongs in production because that's when we can start making predictions at very high scale and low latency. And we can start building new exciting applications using machine learning. That's when the data piece becomes really, really challenging. And the Tecton team was originally at Uber where we built the Michelangelo platform. And Michelangelo is Uber's M&OPS platform. So platform for production ML. And the team's key insight there was that the biggest challenge was dealing with the data. So how do I take raw data, like from batch data sources, streaming data sources, potentially real time data, transform that data into powerful, meaningful signals, also known as features for machine learning, and then some of those features both offline for model training and online for real time influence. That was the biggest challenge that was slowing down machine learning at Uber. And so when we found a Tecton, that was also the problem that we set out to solve. This data problem for production ML. And that's the problem we're focusing on for two reasons. Why? One, because we think it's the biggest challenge in getting machine learning to production. And two, because it's a problem that has up to now really not been addressed directly by companies. And so we are leaving data teams to use existing tooling that's not designed for machine learning, building bespoke custom pipelines. And it's really slowing down the adoption of production machine learning. And so we thought that that was the biggest problem that we could go and solve. And so that's really what Tecton's focused on. Yeah. Fantastic, thank you. Thank you, Guitton. And it was really interesting for me, because we have somewhat similar backgrounds and you kind of came from the infrastructure side and the data side, and then moving into AI and machine learning. But for folks that are out there listening to the show, our audience isn't typically AI and ML experts, right? There's a lot of folks still coming up to speed. And I feel like a terminology lesson is important to level set everyone, right? Because let's start with something like a feature, right? In the enterprise software world, a feature is what it sounds like, but a feature is very different in AI and ML world. And so help everyone out with that a little bit. What is a feature? Why is it important? And then like, let's put it in, because everyone talks to you know, chat GPT these days. How many features to say a chat GPT three or four get trained on? Yeah, yeah. So great questions. So a feature is, you know, very roughly speaking, it is a data signal that you're gonna feed into a machine learning model, right? So it could be, I'll give you an example, right? So if you imagine you use Uber Eats and you order lunch, and you need to get an ETA on when that meal is gonna be delivered. Typically that ETA is generated by machine learning prediction. But it's a complicated problem that has to factor in many data points. So you have to factor in how long does the restaurant typically take to prepare a meal? How complex is this order that you just place? Is it just one meal or is it a meal for 10 people? How busy is the restaurant right now? What is the traffic situation like between the restaurant and your home? Do I have any drivers in the vicinity to deliver the meal? So there's many different data points that have to come together to allow the model to generate a high quality prediction. And those data points or features when a model needs to make a prediction, in the case of Tecton, we'll call out to the Tecton API and say, for example, I need to make a prediction request for Brian or for Ellen. And what we will return is a feature vector. And a feature vector is all the relevant data points that would allow the model to make the prediction for this one person. And what makes it very complicated is that the raw data that is used to generate these features can come from a wide range of sources. They can come from public data sources. They can come from internal company data. It can come from batch data sources. Like it could be data sitting on a data lake or a data warehouse. Could come from a streaming data source like Kafka. Or it could be real time data that is passed to Tecton at the time of prediction, but still needs to be transferred for the model to be able to make a prediction on it. So that's what we call a features. And then going to the second half of your question, how does this relate to large language models or generative AI? Well, generally speaking, we roughly split up the machine learning and AI landscape into three segments. One is everything we call batch or analytical ML. So this is like, I'm generating BI reports. I'm giving some insights to human decision-maker, but I'm gonna use machine learning to provide better insights to the human decision-maker. The second one is production predictive machine learning. This is really what Tecton is focused on, which is to make very high quality, very accurate predictions at very low latency. The use cases of things like, I just mentioned UberEats, but it could be things like fraud detection or product recommendations, imagine you're browsing on Amazon and they wanna give you a recommendation of which products you may like. That is something, again, that you wanna do in real time and needs to be very accurate. So this is what we call predictive machine learning. And then the last area is what I'd call generative AI will win the world of large language models. And so Tecton is really focused on that predictive segment of the market for now. If you think about generative AI, there is definitely a need for using high quality features because the only input you're providing to that large language model is the prompt. And the prompt can be enhanced using high quality data signals or features. So eventually you're gonna see a convergence there between the user features and large language models. But as you think about Chad GPT today, it does not really make use of features in the sense that Tecton understands them because really the data you're providing to the model is all included in that human generated prompt. There's also interesting joint use cases like Stitch Fix, for example, the clothing subscription service has an interesting use case where they use generative AI to understand the feedback that you're giving on the clothes that you will deliver. Like if you send them back and you didn't like them, they're gonna use generative AI to extract the sentiment from that feedback. But then that gets fed into a predictive, those then become features that get fed into a predictive model that will then predict which clothes to send you the next time around. So there's definitely convergence happening between those different segments. But for now Tecton is mostly focused on that predictive machine learning segment. And kind of a follow up to that because we've been talking about features and models, but at the end of the day, it's almost like a tightly coupled pair between the features and I'm gonna use like programming terms like that's the variables or the inputs and the model, which is more like the algorithm or the program, right? Because what you're doing is you're basically putting inputs into the model and one or both of them could be wrong because at the end of the day, like the features and the inputs need to be great and accurate, but the model also needs to interpret everything. So is it kind of a tightly coupling? Is that a correct way to think about all of this? Yes, absolutely. So when the output that's generated by Tecton, there's really two big pieces to it. One is training data and the other side is production, live online influence data. And of course the first step is to train the models and that's essential. Like the accuracy of your predictions will depend completely on both the quality of the model and the quality of the input data, but even the training piece is also very dependent on the quality of the data being useful training. So it's very important to be able to generate highly accurate training data sets, which is actually a pretty complicated thing to do, surprisingly complicated actually, like being able to backfill historical data, maintaining point in time correctness, allowing you to be able to generate any feature vector at any specific point in time, which we call time travel. All that's pretty complicated. These are all problems that Tecton aims to solve as well, because as you mentioned, like the quality of the model is very, very important for that the quality of the training data is essential. Yeah, that's very helpful, your way of explaining it is, maybe you can help me make sure I'm kind of understanding it correctly. I think about, if I draw an analogy to something that most everybody can understand, which is like you go to school, let's say you go to university and you study a major, over a period of time, you're going to have collected a lot of information about that topic. Maybe you're studying business or marketing or something. And that essentially becomes the model that's in your head of what that world looks like, what that corpus of information looks like. And then periodically you will get projects that will say, okay, let's apply your ability to apply that knowledge to specific new context, write about something a certain way or analyze data or something. Is that somewhat of a similar type of analogy of, the ML model is this collection of data that's been sort of trained against certain parameters and then periodically features would be like, I'm throwing a project at you to see how you're going to respond to actually using that data in some real world context. Yes, so yes, I think that's actually, I would equal those recurring projects as retraining a model on new data. So the predictions are actually happening typically at very high velocity, very high volume, low latency. So we can be talking about like hundreds of thousands of queries per second. Like your fraud detection use case, you have every transaction that's happening online will go to a machine learning model to make a prediction on whether this is fraud or not. So this is like the ongoing level of activity of those online models. But then you're also logging every feature that you fed to that model and the response. And then after a while, you can go back in time and be like, well, was the model accurate? Did we make mistakes? And you can use that data to retrain the model. And I'd say it's that the retraining, that loop, that feedback loop that I would equal to the projects you mentioned. Like this is when, hey, now I have a new body of information. I've been running the model for a week. I can go back and look at what would the actual predictions or the errors in predictions. And I can use that to go and make the model better. And then over time, I also may decide to use new features. I could be like, hey, there's an important data point that I was not factoring into the model. So now I'm going to release a new version of the model with more features, which would allow it to hopefully make more accurate predictions. So you're constantly tweaking and optimizing these models. It's a very tight loop and feedback cycle with the data side of things. And Gitan, this actually is a fantastic point because I've kind of always wondered, what exactly does a data scientist do? I don't know that I've ever fully understood that. And by the way, too, there's a couple of blogs in the show notes. So I encourage everyone to take a look at some of the links. But in my mind, is this where a data scientist thinks about that, thinks about what features can be added, thinks about retraining? Are they the ones who turns the data into features and picks the appropriate model and pairs them together? And maybe a more broader question as well, is this where the term feature engineering comes into play? Yeah, yeah, so it's a great question. And I think that the teams and the titles are actually going through a transition right now. So a data scientist, as the title implies, is more of a scientist. So they will indeed do feature engineering. They will basically be given a high level problem to solve. Like detect fraud or help us increase user retention or help us introduce product recommendations. Like if you're Netflix and you wanna introduce what's the next best movie that someone may wanna watch, well, that's a recommendation engine. And so the data scientist is specifically tasked with a business problem, help us solve this business problem with data science. And typically that process involves a lot of data exploration. So figuring out where can I find data that will correlate with this output I'm looking to generate. And so that's feature engineering. It's data exploration, they build features, they will experiment, train models, test those models out, test the accuracy and iterate on that process until they have found a set of features and models that give a good outcome. Now, the thing is data scientists are typically not engineers. And so this process is sufficient if you're just looking at batch machine learning, then we can deploy this model in batch mode and it will generate insights maybe once a day or once a week and everything is good. The challenges we're seeing currently is when we're talking about production ML, like the output coming from the data scientist is typically not ready to go to production. Like they've been experimenting in the Jupyter notebooks on the local machine. And this is not code that is version managed centrally that is ready to go to production. And so they typically hand over that code to a separate team, what we would now call a machine learning engineer. And that team is responsible for taking that input from the data scientists and productionizing it so that it can run at very high volume, low latency, very high SLAs and so forth. And so typically they're gonna re-implement data pipelines using maybe C++ or production level code. And they will get the models to production and deploy them to a model serving platform. That whole process, that handoff process is incredibly inefficient. Like if we think about the software engineering side of things, we can now on the software side, we can release new versions of applications almost on a daily basis. Releasing a new model and new features typically takes months because you're now dependent on this other team that has to re-implement things and some content gets lost in translation and like that team may be busy. So there's a lot of friction in that handoff process. And so the way we think these teams need to evolve and will naturally evolve is that over time, the data scientists will become more like an engineer and will actually build things that are ready to go to production. And that barrier between data science and engineering is going to reduce over time. That's gonna require new tooling. This is precisely one of the things that Tecton aims to solve. Like we talk about DevOps for machine learning data, but there's a whole ecosystem of MLops tools that aim to contribute to solving this problem. And then it's gonna require new processes and kind of a new mindset where data scientists will have to think a little bit more like an engineer and when they're like building models and features, be like, well, is this something I can easily get to production? What's the cost of productionizing this? Can I build something that's actually ready to be deployed in production? And so that's gonna be an interesting evolution over time as we're increasingly running machine learning production that the role of data science and machine learning engineering and hard data evolves over time. Yeah, how does that tend to break down today? So you gave the explanation, you said, look, the business comes to the data scientists and says, look, this is a business problem I want you to solve. How do they go about doing that in terms of, A, obviously they have to establish some domain expertise, right? They have to really kind of understand the steps and whatever's happening in that business process. They have to work with the business to go like, what would you like to know? What would be useful information? Are they the ones who ultimately then are having to collect data, sort of manage an ETL data and so forth, or are they really just kind of defining how information will be collected, what's possible? And then that's handed off to the ML engineer or is there some kind of common thing that a data scientist hands off to that ML engineer and then they have to go optimize it and make it work for production? Yeah, I think, I don't think there's like one standard here. I think it really depends on an organization per organization basis, but in bigger organizations, you will have a data engineering team that is responsible for running all the ETL jobs, getting the data, like clean data in a centralized platform like could be a snowflake data warehouse or a data lake. And then the data scientist job is really to go and discover that data and figure out which data is gonna be best to use to solve that business problem that they've been tasked with. And then the handoff process, and then they're basically iterating in notebooks on local machines, right? And so, and that's typically, I mean, we've seen very large organizations where every data scientist would have like notebooks, the code running in their own notebook, like kind of using different frameworks and processes. And then they would just kind of throw that notebook over to the ML engineering team being like, hey, here you go, put this into production. And this kind of reminds me of the process that we were using in software engineering like 10, 20 years ago, it's really inefficient. And so this is where that process is gonna have to change. We're gonna have to adopt DevOps like best practices. One thing, for example, in the case of Tecton, we centralize all the features that are being used by your organization, right? So imagine you're running a hundred models in production, you may have thousands of features that are being used across those models. Well, those features now become accessible to every data scientist. So instead of having to start from scratch and having to go to re-explore data from scratch, they can actually go to the catalog of existing features, they can browse all the metadata, do searches on the features, explore the data, and very often reuse a large number of features across models. But you can imagine that there's many, many types of machine learning problems that will use very common features across users. How many times has this person ordered something online over the past month? That's a feature that you can imagine being very useful across many different models. And so by centralizing and putting in place some consistent processes, we're allowing these teams to also be a lot more efficient and reuse and share and be able to, for example, we talk about features as code. So in the case of Tecton, a feature exists as a Python file which includes the transformation logic and the feature configuration data. Those feature definitions are managed in a central Git-backed repo. So they're no longer managed in individual notebooks. And so that means that if, as a data scientist, I see a feature that I think I can reuse, but I need to envision it because I need to improve that feature, make some changes, I can just branch it off of the Git repo and make a new feature. And we manage and keep track of all those dependencies and feature versions. And it just allows teams to operate a lot more efficiently. So hopefully that helps answer the question. Yeah, that's super helpful. Thank you, Guitton. Let me ask you this. Because sometimes when you're piecing all of these things together, I can see an organization having trouble with, well, what does success look like? How do they know if the features and model yield a good result just because you got a result? How do you know that it is better? Or how do you know that it is more accurate? And I'm sure there's some kind of baseline measurement. And the answer may be it depends. Every organization is different, but I'm sure there's some characteristics that are typically measured. Yeah, yeah, yeah, yeah. There's cases where accuracy is very easy to measure. For example, the UberEats example I was telling you about, hey, if I forecast that the meal is gonna be delivered at 12.30, but it's actually delivered at 12.40, well, I can immediately get a measure of accuracy. There's other cases where it's a lot more difficult. Fraud detection, you typically don't know right away, but you find out over time and you get, hopefully, a high accuracy on those fraud predictions. But so it really depends. Sometimes it's easy to measure, sometimes it's not. But I think teams still generally struggle to demonstrate the business ROI. And those use cases where it's just so obvious that the ROI is positive. If you can reduce fraud by 50%, that's a huge ROI. Nobody's gonna challenge you on that. There's other use cases where it's like, well, sure, this is probably a better user experience, but what's actually the business value? And an example for this, it's another Tectron customer, it's Atlassian, and Atlassian has the Jira product. Where you're actually creating tickets and assigning tickets to engineers and folks on the team. And the use Tectron to predict and recommend fields when you're actually creating these Jira tickets. And that makes the user experience better because instead of every time having to say, I wanna assign this ticket to Ryan or Ellen, they will tell you, we think this is a ticket that should be assigned to this person. And so it makes the process faster. So it's a better user experience, but it's more difficult to measure the business ROI. What's the actual business outcome of having this capability? I think that's still to some extent an unsolved problem. And so you have to, in those cases, it's really like a PM or a PM-like persona who's making the call of what to invest in. But then, like I mentioned, there's also cases where it's just very, very obvious that the ROI is positive. Yeah. In that context, and kind of following up on that, as you're talking to companies that are building ML models, do you find that, if you ask them, where's your biggest bottleneck? Is it in the handoff from data scientists to the ML engineers, so the velocity at which they can get models into the system, updated, retrained models into the system, or is it on the other side of it where you go, yes, we have these models, they're making predictions, but we can't figure out fast enough if these are good for the business or if they're just new features? Like, is it, does it depend on who you're talking to or are those what feels like the two big bottlenecks today? Yeah, so first off, I think that enterprises and organizations are really understanding that they have to adopt production ML because there's so much data being generated so fast that there is no way that you can keep up with just using traditional BI dashboards and human-in-the-loop decision making. So like going back to Uber, Uber used machine learning to make surge pricing decisions. There's no way they would have an army of people making real-time pricing decisions all over the world. That would just be too expensive. And that's just one example, but I think, many, many organizations are at the point where there's just too much data coming at them too quickly to rely on just traditional BI to realize value from that data. And so at that point, the only way really is to start automating and to automate the simple decisions and production machine learning really is the path to get there. And so I think most organizations recognize that they have to become great at this. In fact, we actually, we just did a survey of like close to 2000 people. And this adoption of applied machine learning was one of the top three priorities for more than 50, for like close to 60% respondents. And now if you ask about like, what are the bottlenecks? What's preventing them to get there? We actually asked them about this too, like what are the biggest challenges? And challenge number one was being able to generate accurate training data. So we talked about this, but like how do I backfill my historical data, be able to like time travel and generate accurate training data sets that would allow me to build highly accurate models. The second biggest challenge was dealing with production data pipelines. And the third one is one we just touched upon, which was demonstrating business ROI. And then there's like a whole slew of other things, right? But I, so I think there's, you know, organizations realize that this is an essential capability. They are investing, they are exploring where, you know, where they're getting the highest ROI, like trying out different use cases and figuring out what's actually having the biggest impact on the business. At the same time, they're figuring out the tooling. So there's a lot of investment going on in the MetOps ecosystem. And many teams are like looking to build the best stack for machine learning. And then, and then thirdly, I think it's like internal processes and people, like figuring out like what's the best organizational model and processes to build these models. And all of this is ongoing. And, you know, I've been at Tecton for about four years, and I have seen like a lot of changes and improvement in the way people think about this production machine learning space. And so it's definitely accelerating. And I think there's still a long ways to go, right? And like the big question is, how long does this transition actually take? Is this like a two year transition or like a 10 year transition? I don't think anybody really knows, but it is definitely moving pretty fast and it's pretty exciting to see the progress we're making. Get on one final question about that, because kind of the way I've been thinking this through is, okay, there was this, as we mentioned, a coupling between the features and the model. And then a lot of times you'll pair those together and almost give that a version number, you know, version three, version four. But if you have, if you, and that's, you know, thinking, again, I'm thinking about it as like, hey, it's trained almost like an offline batch mode, like kind of the more traditional way. But if you're introducing real time data into this, don't you also potentially have a model drift and or the data quality goes down or maybe not that it goes down, but it can't be predicted. You obviously want it to go up, but it could go down as well. And so, so do you have an additional issue there when you introduce real time training into that more traditional coupling? Yeah, I'm glad you asked because like this, this use of real time data is, I think, like one of the biggest challenges. So like a little bit of context on this, you can imagine, like you can be making predictions in a real time, but using batch data, right? So like going back to like the Uber Eats example, you could be making a delivery time prediction based on data from like a week ago, like how long is typically the wait time, how long is typically like the drive time between these locations. But that's gonna give you a prediction, but it's probably not gonna be super accurate. And so quite often when you're doing these real time predictions, you very quickly come to realize that your predictions will be a lot better if you're not just using batch data, but if you're also using some type of real time data. But I'm using real time pretty loosely here, like including streaming data or actual real time data that's passed via API at a time of prediction. But the observation you made is actually very accurate that things get so much more complicated once you're dealing with real time data. Because now you have to train a model based on real time data that you may actually not have yet because the real time hasn't happened, right? So what do you do? Do you start running, do you start logging this real time data and wait for two weeks before you can train your model? That's gonna add two weeks to the project. And so typically what we do is we have a batch data source that backs up in real time data source, but that also adds complexity in terms of maintaining time consistency between these data sources. And so it's a very complicated problem, it needs to like very complicated pipelines and very long implementation times. And so there's absolutely a value to using real time data, but it is also absolutely more complicated. And so you really have to weigh the trade-offs. Like how much more accurate is my model gonna be if I'm using real time data? Maybe if I get a 1% accuracy gain, maybe it's not worth it, right? But then there's other use cases where maybe you get a big accuracy boost. Like if you're doing fraud detection, you're absolutely gonna wanna be using real time data to support fraud detection use cases. So I think it's really, you have to figure this out on a case by case basis. When is real time data really gonna be beneficial? And when it is, then you have to be very careful in your implementation and use the right tooling. And this is one area where technology really shines if you're building real time data pipelines and you're struggling with managing real time features. Well, that's really where we focus most of our time attention. But sort of those tools available out there that will allow you to manage that process because it is pretty complex. Yeah. Before we wrap up, cause I know we're running a little bit short on time, I do wanna highlight and you mentioned it, Aaron mentioned a little bit. Tecton just released a great state of applied machine learning 2023 report. Real quick, we're gonna put a link in the show notes. Folks can definitely go take a look at it. It's always, we always love talking about kind of real data from customers, but real quick, what's the cliff notes or kind of highlights that are coming out of this that people are gonna be a little bit surprised about in terms of how the industry's evolved in this space? Yeah, yeah. I mean, at least for us, it wasn't that, like no huge surprise, but just mostly a confirmation of what we were thinking. So the investment in applied machine learning is a high priority. It is happening in most organizations. So I think we have most organizations having six models in production today, but looking to double that over the next 12 months. So it is, it's definitely being implemented. In terms of the challenges, I mentioned those earlier, like not too surprising, like data continuing to be like the biggest challenge and still taking too long to deploy. Like most organizations take more than a month to deploy and many organizations take more than three months to deploy a new model. And that's really a huge problem because in this space, you're experimenting and you're just figuring out things, fast iteration is a real advantage. Like the faster I can iterate and come up with new versions and fix problems, the better that's gonna be for the organization. And so like this slow speed of deployment is a huge concern. The MLOps tooling ecosystem is actually being implemented quite quickly, which was a great kind of a surprise to us. Like, you know, we sometimes feel like, hey, many organizations are not quite mature enough to use these more advanced MLOps tools, but it's actually not the case. If I look at the data, we kind of split out the MLOps ecosystem in five different components. And I'd say it's probably around 30 to 40% of organizations are using each of these components on average, but that's going to go up to like 78% by next year. So I think the option of the MLOps tooling ecosystem is happening pretty quickly. And I think those are the key things. Yeah, maybe on the use cases very quickly, the key use cases or things like I just go from like the most popular to the least popular, but number one was recommender systems. Number two was customer analytics. Number three, personalization. Number four, research and ranking. Number five is risky analytics. Number six is fraud detection. And number seven, dynamic pricing. So those are the main use cases where applied MLO is being used today. And I think those are the key insights. Yeah. Excellent, excellent, excellent stuff. Well, Gaetan, thank you so much for the insight, not only into how this all works and so forth, but also just being patient and putting up with some of the most basic questions that Aaron and I were asking. So we appreciate your insight both at the service level and in depth. Yeah, for sure. Thanks for having me and thanks for asking great questions. I think we need more education in this space. So we're really happy to be able to hopefully help shed some light on some of these foundational aspects. Yeah, good stuff, good stuff. Aaron, you want to wrap it up and take us home? Yeah, absolutely. So thank you everyone for listening this week. I know we went a little bit long, but Brian and I were actually texting in the back room and we're like, we're going to let this one go a little long because I felt like the features and the background in features and getting some of the terminology, hopefully that was super helpful to y'all. And as always, give us feedback on that. Certainly leave us a review wherever you get your podcasts if you have that ability to do so. And of course, we certainly hope it's five-star reviews. And also if you have feedback on guests, we're always looking for more guests. It's show at thecloudcast.net is the best way to get in touch with us these days. I think Brian might still also check the Twitter account. Honestly, I'm not on Twitter, but maybe Brian. Yeah, we're everywhere, we're everywhere. Twitter, TikTok, we got to be everywhere these days. Oh gosh, it's such a pain. Streams, TikTok, you name it, we're there. So hopefully you can get hold of us pretty much everywhere these days. But again, everyone, thanks for listening. We appreciate your time and we will talk to everyone next week. Thank you for listening to the Cloudcast. Please visit thecloudcast.net to find more shows, show notes, videos, and everything social media. And we'll see you next time. Bye."}, "podcast_summary": "Host Name: Aaron Delp and Brian Gracely\n\nPodcast Title: Understanding Machine Learning Features and Platforms with Gaitan Castillon from Tecton\n\nIn this episode, Aaron and Brian discuss machine learning features and platforms with Gaitan Castillon from Tecton. They explain the concept of features in machine learning and how they are used in predictive models. They also discuss the challenges of transitioning from batch machine learning to production machine learning and the importance of accurate training data. Overall, the episode provides valuable insights into the world of machine learning and the evolving role of data scientists and ML engineers.", "podcast_guest": "Gaitan Casalon", "podcastGuest": "Not Available", "podcast_guest_org": "Tecton", "podcast_highlights": "The key topics and their corresponding timestamps are:\n\nTimestamp: 01:52 - HashiCorp's adoption of the BSL license\nTimestamp: 02:55 - NVIDIA's partnership with Hugging Face\nTimestamp: 04:22 - Defining, measuring, and managing technical debt\nTimestamp: 08:10 - Introduction to Gaitan Kastelan and Tecton\nTimestamp: 10:51 - Understanding machine learning features and platforms\nTimestamp: 13:43 - The role of a data scientist in building ML models\nTimestamp: 15:54 - Challenges in the handoff between data scientists and ML engineers\nTimestamp: 17:16 - Measuring the success and accuracy of ML models\nTimestamp: 19:04 - Bottlenecks in ML model deployment and ROI demonstration\nTimestamp: 21:01 - The use of real-time data in ML models\nTimestamp: 23:44 - Highlights from the State of Applied Machine Learning 2023 report"}